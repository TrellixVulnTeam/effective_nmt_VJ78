{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import Encoder, Decoder, NMT\n",
    "from vocab import Vocab\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect our data\n",
    "language = 'es'\n",
    "tarfilename = \"{}-en.tgz\".format(language)\n",
    "tarfilepath = os.path.exists(os.path.join(\"data/\", tarfilename))\n",
    "def maybe_download():\n",
    "    if not os.path.exists(tarfilepath):\n",
    "        print('downloading {}...'.format(tarfilename))\n",
    "        url = \"http://www.statmt.org/europarl/v7/{}\".format(tarfilename)\n",
    "        os.makedirs('data/', exist_ok=True)\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(tarfile, 'wb') as fd:\n",
    "            for content in tqdm(r.iter_content()):\n",
    "                fd.write(content)\n",
    "        print('download complete! Extracting...')\n",
    "        with tarfile.open(tarfilepath) as tar:\n",
    "            tar.extractall(path='data/')\n",
    "        print('done!')\n",
    "        \n",
    "maybe_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data data data\n",
    "englishfile = 'data/europarl-v7.es-en.en'\n",
    "spanishfile = 'data/europarl-v7.es-en.es'\n",
    "\n",
    "def build_full_vocabs():\n",
    "    with open(englishfile) as en_fd, open(spanishfile) as es_fd:\n",
    "        en_lang = Vocab(name='english')\n",
    "        es_lang = Vocab(name='spanish')\n",
    "        try:\n",
    "            en_lang.add_corpus(en_fd)\n",
    "        except VocabFull:\n",
    "            pass\n",
    "        try:\n",
    "            es_lang.add_corpus(es_fd)\n",
    "        except:\n",
    "            pass\n",
    "    en_lang.calcify()\n",
    "    es_lang.calcify()\n",
    "    return en_lang, es_lang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def corpora2vectors():\n",
    "    with open(englishfile) as en_fd, open(spanishfile) as es_fd:\n",
    "        eng = [en_lang.tokens2tensor(en_lang.word_tokenize(s)) for s in en_fd]\n",
    "        es = [es_lang.tokens2tensor(es_lang.word_tokenize(s)) for s in es_fd]\n",
    "    return eng, es\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lang, es_lang = build_full_vocabs()\n",
    "X, y = corpora2vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, name: str, vocab: Vocab, embedding_size: int,\n",
    "                 n_hidden: int, lstm_layers: int):\n",
    "        \"\"\"\n",
    "        Basic Encoder Module for neural machine translation\n",
    "        :param name: module name\n",
    "        :param vocab: Vocab object, for mapping numerics to words\n",
    "        :param embedding_size: size of word embedding\n",
    "        :param n_hidden: number of hidden nodes in each lstm.\n",
    "        :param lstm_layers: number of lstm layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.__name__ = name\n",
    "\n",
    "        # Saving this so that other parts of the class can re-use it\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = lstm_layers\n",
    "\n",
    "        # word embeddings:\n",
    "        self.input_lookup = nn.Embedding(num_embeddings=vocab.size,\n",
    "                                         embedding_dim=embedding_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size,\n",
    "                            hidden_size=self.n_hidden,\n",
    "                            num_layers=self.n_layers,\n",
    "                            bidirectional=False)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\n",
    "        basic forward pass of encoder, defined for full inputs\n",
    "        :param input: encoder lstm inputs\n",
    "        :param hidden: previous lstm hidden state\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        embedding = self.input_lookup(input)\n",
    "        output = embedding\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        h0 = torch.zeros(self.n_layers, 1, self.n_hidden).cuda()\n",
    "        c0 = torch.zeros(self.n_layers, 1, self.n_hidden).cuda()\n",
    "        return h0, c0\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, name: str, vocab: Vocab, embedding_size: int,\n",
    "                 n_hidden: int, lstm_layers: int, local_window: int):\n",
    "        \"\"\"\n",
    "        Decoder Module with Attention Mechanism for neural machine translation\n",
    "        :param name: name for object instance\n",
    "        :param vocab: output vocabulary for predictions\n",
    "        :param embedding_size:\n",
    "        :param n_hidden:\n",
    "        :param lstm_layers:\n",
    "        :param local_window:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.__name__ = name\n",
    "\n",
    "        n_pt_weights = n_hidden\n",
    "\n",
    "        # Saving this so that other parts of the class can re-use it\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = lstm_layers\n",
    "        self.out_lut = vocab.tensor2tokens\n",
    "        self.local_window = local_window\n",
    "\n",
    "        # word embeddings:\n",
    "        self.output_lookup = nn.Embedding(num_embeddings=vocab.size,\n",
    "                                          embedding_dim=embedding_size)\n",
    "\n",
    "        # attention module\n",
    "        self.p_t_dense = nn.Linear(self.n_hidden, n_pt_weights, bias=False)\n",
    "        self.p_t_dot = nn.Linear(n_pt_weights, 1, bias=False)\n",
    "        self.score = nn.Bilinear(self.n_hidden, self.n_hidden, 1, bias=False)  # ?\n",
    "\n",
    "        self.combine_attention = nn.Linear(2 * self.n_hidden, self.n_hidden)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size + self.n_hidden,\n",
    "                            hidden_size=self.n_hidden,\n",
    "                            num_layers=self.n_layers,\n",
    "                            bidirectional=False,\n",
    "                            dropout=0.2)\n",
    "\n",
    "        self.dense_out = nn.Linear(self.n_hidden, vocab.size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input, hidden, h_s, h_t_tilde):\n",
    "        embedding = self.output_lookup(input.view(1, 1))\n",
    "\n",
    "        context_embedding = torch.cat((embedding, h_t_tilde), dim=-1)\n",
    "\n",
    "        # lstm\n",
    "        output, hidden = self.lstm(context_embedding, hidden)\n",
    "\n",
    "        # attention\n",
    "        if self.local_window:\n",
    "            # local\n",
    "            h_t = hidden[0][-1]\n",
    "            p_t = h_s.size(0) * torch.sigmoid(self.p_t_dot(torch.tanh(self.p_t_dense(h_t)))).squeeze()  # (9)\n",
    "            s = torch.round(p_t).long().cuda()\n",
    "            D = self.local_window\n",
    "            minimum, maximum = max(s - D, 0), min(s + D, h_s.size(0) - 1)\n",
    "            h_s_local = h_s[minimum:maximum + 1]  # @@@@@ zero pad? @@@@@\n",
    "            h_t_rep = h_t.repeat(h_s_local.size(0), 1, 1)\n",
    "            score = self.score(h_t_rep, h_s_local)  # (8) (general)\n",
    "            gauss_window = torch.exp((torch.arange(minimum, maximum + 1).float() - p_t) ** 2 / (D / 2) ** 2).view(-1, 1, 1).cuda()\n",
    "            a_t = torch.softmax(score, dim=0) * gauss_window  # (7) & (10)\n",
    "            context = torch.mean(a_t * h_s_local, dim=0, keepdim=True)\n",
    "        else:\n",
    "            # global\n",
    "            pass\n",
    "\n",
    "        h_t_tilde = torch.tanh(self.combine_attention(torch.cat((context, h_t.view(1, 1, -1)), dim=-1)))  # (5)\n",
    "        y = self.softmax(self.dense_out(h_t_tilde))  # (6)\n",
    "\n",
    "        return y, hidden, h_t_tilde, (a_t, s)\n",
    "\n",
    "\n",
    "class NMT:\n",
    "\n",
    "    def __init__(self, name, encoder, decoder, loss_fn):\n",
    "        \"\"\"\n",
    "        simple wrapper class for handling encoder-decoder structure.\n",
    "        :param name: Name for NMT instance\n",
    "        :param encoder: Encoder module instance\n",
    "        :param decoder: Decoder module instance (must match hidden state size of encoder)\n",
    "        :param loss_fn: a loss function for comparison with log-softmax layer. Probably NLLLoss(dim=-1)\n",
    "        \"\"\"\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optim.Adam((*encoder.parameters(), *decoder.parameters()), lr=1e-3)\n",
    "        self.epochs = 0\n",
    "\n",
    "    def set_optimizer(self, optimizer, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        explicitly set the optimizer\n",
    "        :param optimizer: new optimizer\n",
    "        :param learning_rate: new learning rate\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer((*self.encoder.parameters(),\n",
    "                                    *self.decoder.parameters()),\n",
    "                                   lr=learning_rate)\n",
    "\n",
    "    def train(self, X, y, epochs=1, batch_size=1, clipping=0.25, print_every=4, examples=[],\n",
    "              examples_epoch_fn=lambda x: True):\n",
    "        \"\"\"\n",
    "        epoch training function\n",
    "        :param X: input training data\n",
    "        :param y: output training data\n",
    "        :param epochs: number of epochs to train\n",
    "        :param batch_size: batch size (WARNING: Empirically doesn't work)\n",
    "        :param clipping: gradient clipping coef\n",
    "        :param print_every: print loss at every N% completion of each epoch\n",
    "        :param examples: optional training examples for displaying progress.\n",
    "        :param examples_epoch_fn: function which takes the epoch number, and returns a boolean value.\n",
    "        Training examples are only printed for epochs where the function returns true.\n",
    "        e.g. lambda x: x**(1/2) == int(x**(1/2))\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        N = len(X)\n",
    "\n",
    "        for e in range(epochs):\n",
    "            print_loss_total = prev_log_iter = prev_log_percent = 0\n",
    "            for n in range(N):\n",
    "                xi = X[n]\n",
    "                yi = y[n]\n",
    "                step = not n % batch_size or n == N - 1\n",
    "                print_loss_total += self._train(xi, yi, clipping, step=step, loss_factor=batch_size) * batch_size\n",
    "                if step and 100 * n // N > prev_log_percent + print_every:\n",
    "                    print_loss_avg = print_loss_total / (n - prev_log_iter)\n",
    "                    prev_log_iter = n\n",
    "                    prev_log_percent = 100 * n // N\n",
    "                    print('\\repoch %s: %%%s complete     avg loss: %.4f' %\n",
    "                          (self.epochs, prev_log_percent, print_loss_avg))\n",
    "                    print_loss_total = 0\n",
    "            self.epochs += 1\n",
    "            if examples_epoch_fn(e):\n",
    "                for ex in examples:\n",
    "                    output = self.predict(ex)\n",
    "                    print(' '.join(self.decoder.out_lut(output)))\n",
    "\n",
    "    def predict(self, input, cap: int = 20):\n",
    "        \"\"\"\n",
    "        returns prediction vector from input\n",
    "        :param input: input statement\n",
    "        :param cap: max length for prediction.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        hidden = self.encoder.init_hidden()\n",
    "        encoded, hidden = self.encoder(input.view(-1, 1), hidden)\n",
    "        inp = torch.tensor([[1]]).cuda()  # EOS_TOKEN\n",
    "        h_t_tilde = hidden[0][-1].unsqueeze(0) * 0\n",
    "        output = []\n",
    "        for di in range(cap):\n",
    "            logits, hidden, h_t_tilde, decoder_attention = self.decoder(inp, hidden, encoded, h_t_tilde)\n",
    "            topv, topi = logits.topk(1)\n",
    "            inp = topi.squeeze().detach()  # detach from history as input\n",
    "            if inp.item() == 1:  # EOS_TOKEN\n",
    "                break\n",
    "            output.append(inp.squeeze())\n",
    "\n",
    "        return torch.stack(tuple(output), dim=0)\n",
    "\n",
    "    def save(self, path=None):\n",
    "        if not path:\n",
    "            path = os.path.join('.', self.name)\n",
    "        encoder = os.path.join(path, '_encoder')\n",
    "        decoder = os.path.join(path, '_decoder')\n",
    "        torch.save(self.encoder.state_dict(), encoder)\n",
    "        torch.save(self.decoder.state_dict(), decoder)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path, name='nmt'):\n",
    "        encoder = os.path.join(path, '_encoder')\n",
    "        decoder = os.path.join(path, '_decoder')\n",
    "        enc = torch.load(encoder)\n",
    "        dec = torch.load(decoder)\n",
    "        encoder = Encoder.load_state_dict(enc)\n",
    "        decoder = Decoder.load_state_dict(dec)\n",
    "        nmt = NMT(name, encoder, decoder, loss_fn=nn.NLLLoss())\n",
    "        return nmt\n",
    "\n",
    "    def _train(self, x, y, clipping, step, loss_factor=1):\n",
    "        \"\"\"\n",
    "        single example training\n",
    "        :param x: single input sentence vector\n",
    "        :param y: single output sentence vector\n",
    "        :param clipping: gradient clipping\n",
    "        :param step: execute optimizer step or not\n",
    "        :param loss_factor: basic factor for batch averaging.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        hidden = self.encoder.init_hidden()\n",
    "\n",
    "        target_length = y.size(0)\n",
    "\n",
    "        loss = torch.tensor(0)\n",
    "        x = x.view(-1, 1)\n",
    "        h_s, hidden = self.encoder(x, hidden)\n",
    "\n",
    "        bos = torch.tensor(\n",
    "            [[0]]).cuda()  # BOS_TOKEN\n",
    "        inp = bos\n",
    "        h_t_tilde = hidden[0][-1].unsqueeze(0) * 0\n",
    "        for di in range(target_length):\n",
    "            logits, hidden, h_t_tilde, decoder_attention = self.decoder(inp, hidden, h_s, h_t_tilde)\n",
    "            topv, topi = logits.topk(1)\n",
    "            inp = topi.squeeze().detach()  # detach from history as input\n",
    "            loss += self.loss_fn(logits.view(1, -1), y[di].view(1)) / loss_factor\n",
    "            if inp.item() == 1:  # EOS_TOKEN\n",
    "                break\n",
    "            inp = y[di]\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_((*self.encoder.parameters(), *self.decoder.parameters()), clipping)\n",
    "\n",
    "        if step:\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        return loss.item() / target_length\n",
    "\n",
    "\n",
    "class VanillaDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, name: str, vocab: Vocab, embedding_size: int,\n",
    "                 n_hidden: int, lstm_layers: int, local_window: int):\n",
    "        \"\"\"\n",
    "        Decoder Module without an attention mechanism, for comparison.\n",
    "        See Decoder for parameter help.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.__name__ = name\n",
    "\n",
    "        n_pt_weights = n_hidden\n",
    "        self.lut = vocab.tokens2tensor\n",
    "\n",
    "        # Saving this so that other parts of the class can re-use it\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = lstm_layers\n",
    "        self.local_window = local_window\n",
    "\n",
    "        # word embeddings:\n",
    "        self.output_lookup = nn.Embedding(num_embeddings=vocab.size,\n",
    "                                          embedding_dim=embedding_size)\n",
    "\n",
    "        # attention module\n",
    "        self.p_t_dense = nn.Linear(self.n_hidden, n_pt_weights, bias=False)\n",
    "        self.p_t_dot = nn.Linear(n_pt_weights, 1, bias=False)\n",
    "        self.score = nn.Bilinear(self.n_hidden, self.n_hidden, 1, bias=False)  # ?\n",
    "\n",
    "        self.combine_attention = nn.Linear(2 * self.n_hidden, self.n_hidden)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size,  # + self.n_hidden\n",
    "                            hidden_size=self.n_hidden,\n",
    "                            num_layers=self.n_layers,\n",
    "                            bidirectional=False,\n",
    "                            dropout=0.2)\n",
    "\n",
    "        self.dense_out = nn.Linear(self.n_hidden, vocab.size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input, hidden, h_s, h_t_tilde):\n",
    "        embedding = self.output_lookup(input.view(1, 1))\n",
    "        # context_embedding = torch.cat((embedding, h_t_tilde), dim=-1)\n",
    "\n",
    "        # lstm\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "\n",
    "        y = self.softmax(self.dense_out(output))\n",
    "        return y, hidden, h_t_tilde, \\\n",
    "            (None, None)  # for conformed interfacing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = zip(*((i, j) for i, j in zip(X, y) if len(i) and len(j) != 1))\n",
    "X = list(X)\n",
    "y = list(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "encoder = Encoder('encoder', vocab=en_lang, embedding_size=70, n_hidden=70, lstm_layers=2)\n",
    "decoder = Decoder('decoder', vocab=es_lang, embedding_size=70, n_hidden=70, lstm_layers=2, local_window=2)\n",
    "encoder = encoder.cuda()\n",
    "decoder = decoder.cuda()\n",
    "\n",
    "# cudafy inputs\n",
    "for i, (xi, yi) in enumerate(zip(X_train, y_train)):\n",
    "    X_train[i] = xi.cuda()\n",
    "    y_train[i] = yi.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt = NMT('local_nmt', encoder, decoder, nn.NLLLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with type torch.cuda.LongTensor doesn't match the desired type torch.cuda.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-50054e65c4a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-822466f53559>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, epochs, batch_size, clipping, print_every, examples, examples_epoch_fn)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0myi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclipping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mprev_log_percent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mprint_loss_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev_log_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-822466f53559>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, x, y, clipping, step, loss_factor)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# detach from history as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mloss_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# EOS_TOKEN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with type torch.cuda.LongTensor doesn't match the desired type torch.cuda.FloatTensor"
     ]
    }
   ],
   "source": [
    "nmt.train(X_train, y_train, epochs=100, batch_size=1, examples=X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import Encoder, Decoder\n",
    "from vocab import Vocab, VocabFull\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect our data\n",
    "language = 'es'\n",
    "tarfilename = \"{}-en.tgz\".format(language)\n",
    "tarfilepath = os.path.exists(os.path.join(\"data/\", tarfilename))\n",
    "def maybe_download():\n",
    "    if not os.path.exists(tarfilepath):\n",
    "        print('downloading {}...'.format(tarfilename))\n",
    "        url = \"http://www.statmt.org/europarl/v7/{}\".format(tarfilename)\n",
    "        os.makedirs('data/', exist_ok=True)\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(tarfile, 'wb') as fd:\n",
    "            for content in tqdm(r.iter_content()):\n",
    "                fd.write(content)\n",
    "        print('download complete! Extracting...')\n",
    "        with tarfile.open(tarfilepath) as tar:\n",
    "            tar.extractall(path='data/')\n",
    "        print('done!')\n",
    "        \n",
    "maybe_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data data data\n",
    "englishfile = 'data/europarl-v7.es-en_trunc.en'\n",
    "spanishfile = 'data/europarl-v7.es-en_trunc.es'\n",
    "\n",
    "def build_full_vocabs():\n",
    "    with open(englishfile) as en_fd, open(spanishfile) as es_fd:\n",
    "        en_lang = Vocab(name='english')\n",
    "        es_lang = Vocab(name='spanish')\n",
    "        try:\n",
    "            en_lang.add_corpus(en_fd)\n",
    "        except VocabFull:\n",
    "            pass\n",
    "        try:\n",
    "            es_lang.add_corpus(es_fd)\n",
    "        except:\n",
    "            pass\n",
    "    en_lang.calcify()\n",
    "    es_lang.calcify()\n",
    "    return en_lang, es_lang\n",
    "\n",
    "if True:\n",
    "    en_lang, es_lang = build_full_vocabs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def corpora2vectors():\n",
    "    with open(englishfile) as en_fd, open(spanishfile) as es_fd:\n",
    "        eng = [en_lang.tokens2tensor(en_lang.word_tokenize(s)) for s in en_fd]\n",
    "        es = [es_lang.tokens2tensor(es_lang.word_tokenize(s)) for s in es_fd]\n",
    "    return eng, es\n",
    "\n",
    "if True:\n",
    "    X, y = corpora2vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

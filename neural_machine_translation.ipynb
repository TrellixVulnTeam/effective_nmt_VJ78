{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "from nltk.translate import bleu_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import Encoder, Decoder, NMT\n",
    "from vocab import Vocab\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Attention in Neural Machine Translation\n",
    "\n",
    "A pytorch implementation of [\"Effective Approaches to Attention-based Neural Machine Translation\" by Luong, Pham, and Manning (2015).](https://arxiv.org/abs/1508.04025)\n",
    "\n",
    "The first three sections of this text will be exposition, and can be skimmed over or skipped entirely if you'd like to go straight to the methods and/or results section.\n",
    "\n",
    "##### Foreward\n",
    "\n",
    "##### Introduction\n",
    "  - Languages\n",
    "  - Machine Translation\n",
    "\n",
    "##### Background\n",
    "  - Seq2Seq\n",
    "  - Neural Machine Translation\n",
    "  - Attention Mechanisms\n",
    "\n",
    "##### Methods\n",
    "  - LSTMs (seq2seq)\n",
    "  - Attention Mechanisms\n",
    "    - global\n",
    "    - local\n",
    "  - Europarl (English-Spanish)\n",
    "\n",
    "##### Results\n",
    "  - Vanilla seq2seq\n",
    "  - Attention\n",
    "    - global\n",
    "    - local\n",
    "\n",
    "##### Conclusion\n",
    "\n",
    "\n",
    "## Foreward\n",
    "\n",
    "> *If you think a good translation is expensive, you should look at the cost of a bad translation*\n",
    "\n",
    "I'm not much of a language translator myself, though I like to think of myself as a pretty good translator of mathematics and physics. In the domain of natural languages, I just know conversational Spanish, however probably learned too late in life to ever hope to translate with any effectiveness. Depending on the how and when, learning languages is both the most effortless and one of the most difficult tasks for humans. \n",
    "\n",
    "Translation between languages is a notoriously difficult task, even for humans. If you are an avid reader, you may have come across texts translated from another language. Frequently, if the work is old and popular enough, there can be much deliberation over which translation does the original work the most justice. In the abstract we can think of language as semantic descriptions of the world, even if on a deeper level, they are expressions of human ideas.\n",
    "\n",
    "The translation of languages using computers starts in the 1930s with translation between English and Russian.\n",
    "\n",
    "As it is my one extra language, we'll be exploring translation of English phrases into Spanish phrases. We have a nice Spanish phrase to illustrate some of our difficulties in moving forward:\n",
    "\n",
    "\n",
    "*Es un idioma*, generally translates to in English: *It's a language*\n",
    "\n",
    "However, it can also be used to mean: *It's an idiom*, a figurative phrase which is difficult to translate.\n",
    "\n",
    "So it's also used as a dismissal for such difficult to translate phrases, 'I can't really translate that... *Es un idioma*'\n",
    "\n",
    "I think it's a self-aware symbolism that these Spanish homonyms mean either *idiom* or *language*.\n",
    "\n",
    "With that in mind, let's get started!\n",
    "\n",
    "##### Introduction\n",
    "\n",
    "\n",
    "\n",
    "## Background\n",
    "\n",
    "The sequence to sequence (seq2seq) architecture is a common RNN pattern which allows for variable length input and output layers. It allows a network to learn both an embedding for an input sequence, and to emit an output sequence from that embedding. These tasks are given to two separate RNN units or networks, which we will call the \"encoder\" and the \"decoder\". This architecture is well suited to machine translation tasks, as sentences vary in length within languages and between language translations. A vocabulary is commonly constructed for each language in order for the network to represent the words of each language as a finite collection. First, the input vocabulary will be transformed into its integer representation (or a one-hot encoding), and fed into the encoder.\n",
    "\n",
    "![encoder-decoder](img/lstm.png)\n",
    "\n",
    "[figure found online](https://smerity.com/articles/2016/google_nmt_arch.html)\n",
    "\n",
    "The architecture above displays this process for English-to-German. For each word in the English phrase, the encoder takes the input feature vector, and passes it along via its hidden state, as shown on the left. The task of each time step of the decoder, is to output/emit a layer which is then fed into a softmax activation, as shown on the right. This softmax represents our probabilities or logits in the space of our output vocabulary.\n",
    "\n",
    "##### Attention\n",
    "\n",
    "Attention mechanisms in deep learning are an effective method of addressing *vanishing gradients* as well as assisting learning in highly overparameterized or large input models. The name suggests the relationship with human experience, where the network is allowed to *focus* on a subset of a layer, by masking or weighting the contributions of a layer. Some attention mechanisms are stochastic, and non-differentiable. Here we are only concerned with differentiable attention on LSTMs.\n",
    "\n",
    "An attention mechanism is said to be *global* where it acts as a mask or smooth (i.e. weighted) mask across an entire layer. This addresses the problems stated above, and allows the model to converge. However, it can increase the training time significantly by demanding a large number of (generally trivial) computations. A *local* attention method resolves this problem by somehow choosing which inputs to consider, instead of zeroing out a large number of inputs. The challenge with local methods is to conceive of a *good*, *differentiable* local attention mechanism.\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "##### Encoder-Decoder Network\n",
    "\n",
    "![lstm](img/lstm_diagram.png)\n",
    "Luong et al. Figure 1\n",
    "\n",
    "For our basic network we use the seq2seq model with identical bidirectional LSTM networks for the encoder and decoder, as depicted in the figure above. This figure depicts a forward pass of the model during testing, where the  input which is fed into the decoder each step is the output of the previous step, with the exception of a start token. \n",
    "\n",
    "During training, by contrast, the $X', Y', Z'$ predictions of the decoder are used to compute the loss, however the inputs are determined by the ground truth labels. This is especially important for early convergence, where the output sequence will be too chaotic to learn from. Eventually weaning the network off of ground truth sequences as inputs might be a good idea to further improve model results.\n",
    "\n",
    "##### Global Attention\n",
    "\n",
    "![global attention](img/lstm_global_attention_diagram.png)\n",
    "Luong et al. Figure 2.\n",
    "\n",
    "The figure about depicts the global attention mechanism. The vector $a_t$, the attention vector, is computed from the hidden states across the entirety of the input sequence.\n",
    "\n",
    "##### Local Attention\n",
    "\n",
    "![local attention](img/lstm_local_attention_diagram.png)\n",
    "\n",
    "The figure about depicts the local attention mechanism. The vector $a_t$, the attention vector, is computed from the hidden states across a small window of the input sequence.\n",
    "\n",
    "\n",
    "The input layer size and hidden layer size were chosen arbitrarily, but in principle can be heuristically optimized using cross-validation methods.\n",
    "\n",
    "To train the model, we use the [europarl](http://www.statmt.org/europarl/) corpus, a parallel corpus of translated proceedings of the European Parliament from 1996-2011. The corpus contains translations between 21 European languages, and has already (mostly) been preprocessed.\n",
    "\n",
    "The Spanish/English pairs are downloaded using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect our data\n",
    "language = 'es'\n",
    "tarfilename = \"{}-en.tgz\".format(language)\n",
    "tarfilepath = os.path.join(\"data/\", tarfilename)\n",
    "englishfile = 'data/europarl-v7.{}-en.en'.format(language)\n",
    "spanishfile = 'data/europarl-v7.{0}-en.{0}'.format(language)\n",
    "def maybe_download():\n",
    "    if not os.path.exists(tarfilepath):\n",
    "        print('downloading {}...'.format(tarfilename))\n",
    "        url = \"http://www.statmt.org/europarl/v7/{}\".format(tarfilename)\n",
    "        os.makedirs('data/', exist_ok=True)\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(tarfilepath, 'wb') as fd:\n",
    "            for content in r.iter_content():\n",
    "                fd.write(content)\n",
    "        print('download complete!')\n",
    "    if not os.path.exists(englishfile):\n",
    "        print('Extracting...')\n",
    "        with tarfile.open(tarfilepath) as tar:\n",
    "            tar.extractall(path='data/')\n",
    "        print('done!')\n",
    "        \n",
    "maybe_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data data data\n",
    "\n",
    "def build_full_vocabs():\n",
    "    with open(englishfile) as en_fd, open(spanishfile) as es_fd:\n",
    "        en_lang = Vocab(name='english')\n",
    "        es_lang = Vocab(name='spanish')\n",
    "        try:\n",
    "            en_lang.add_corpus(en_fd)\n",
    "        except VocabFull:\n",
    "            pass\n",
    "        try:\n",
    "            es_lang.add_corpus(es_fd)\n",
    "        except:\n",
    "            pass\n",
    "    en_lang.calcify()\n",
    "    es_lang.calcify()\n",
    "    return en_lang, es_lang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def corpora2vectors():\n",
    "    with open(englishfile) as en_fd, open(spanishfile) as es_fd:\n",
    "        eng = []\n",
    "        esp = []\n",
    "        reg = re.compile('[a-zA-Z]')\n",
    "        for en, es in zip(en_fd, es_fd):\n",
    "            en = en.strip()\n",
    "            es = es.strip()\n",
    "            if not reg.search('[a-zA-Z]', en):\n",
    "                continue\n",
    "            if not reg.search('[a-zA-Z]', es):\n",
    "                continue\n",
    "            eng.append(en_lang.tokens2tensor(en_lang.word_tokenize(en)))\n",
    "            esp.append(es_lang.tokens2tensor(es_lang.word_tokenize(es)))\n",
    "    return eng, es\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't rebuild corpus\n",
    "if os.path.exists('X.pkl'):\n",
    "    with open('en_lang.pkl', 'rb') as fd:\n",
    "        en_lang = pickle.load(fd)\n",
    "    with open('es_lang.pkl', 'rb') as fd:\n",
    "        es_lang = pickle.load(fd)\n",
    "    #with open('X.pkl', 'rb') as fd:\n",
    "    #    X = pickle.load(fd)\n",
    "    #with open('y.pkl', 'rb') as fd:\n",
    "    #    y = pickle.load(fd)\n",
    "else:\n",
    "    en_lang, es_lang = build_full_vocabs()\n",
    "    X, y = corpora2vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, name: str, vocab: Vocab, embedding_size: int,\n",
    "                 n_hidden: int, lstm_layers: int):\n",
    "        \"\"\"\n",
    "        Basic Encoder Module for neural machine translation\n",
    "        :param name: module name\n",
    "        :param vocab: Vocab object, for mapping numerics to words\n",
    "        :param embedding_size: size of word embedding\n",
    "        :param n_hidden: number of hidden nodes in each lstm.\n",
    "        :param lstm_layers: number of lstm layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.__name__ = name\n",
    "\n",
    "        # Saving this so that other parts of the class can re-use it\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = lstm_layers\n",
    "\n",
    "        # word embeddings:\n",
    "        self.input_lookup = nn.Embedding(num_embeddings=vocab.size,\n",
    "                                         embedding_dim=embedding_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size,\n",
    "                            hidden_size=self.n_hidden,\n",
    "                            num_layers=self.n_layers,\n",
    "                            bidirectional=False)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\"\n",
    "        basic forward pass of encoder, defined for full inputs\n",
    "        :param input: encoder lstm inputs\n",
    "        :param hidden: previous lstm hidden state\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        embedding = self.input_lookup(input)\n",
    "        output = embedding\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        h0 = torch.zeros(self.n_layers, 1, self.n_hidden).cuda()\n",
    "        c0 = torch.zeros(self.n_layers, 1, self.n_hidden).cuda()\n",
    "        return h0, c0\n",
    "\n",
    "class VanillaDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, name: str, vocab: Vocab, embedding_size: int,\n",
    "                 n_hidden: int, lstm_layers: int):\n",
    "        \"\"\"\n",
    "        Decoder Module without an attention mechanism, for comparison.\n",
    "        See Decoder for parameter help.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.__name__ = name\n",
    "        \n",
    "        # some useful static properties\n",
    "        n_pt_weights = n_hidden\n",
    "        self.lut = vocab.tokens2tensor\n",
    "        self.out_lut = vocab.tensor2tokens\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = lstm_layers\n",
    "\n",
    "        # embedding:\n",
    "        self.output_lookup = nn.Embedding(num_embeddings=vocab.size,\n",
    "                                          embedding_dim=embedding_size)\n",
    "\n",
    "        # attention module\n",
    "        #self.p_t_dense = nn.Linear(self.n_hidden, n_pt_weights, bias=False)\n",
    "        #self.p_t_dot = nn.Linear(n_pt_weights, 1, bias=False)\n",
    "        #self.score = nn.Bilinear(self.n_hidden, self.n_hidden, 1, bias=False)  # ?\n",
    "\n",
    "        #self.combine_attention = nn.Linear(2 * self.n_hidden, self.n_hidden)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size,  # + self.n_hidden\n",
    "                            hidden_size=self.n_hidden,\n",
    "                            num_layers=self.n_layers,\n",
    "                            bidirectional=False,\n",
    "                            dropout=0.2)\n",
    "\n",
    "        self.dense_out = nn.Linear(self.n_hidden, vocab.size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input, hidden, h_s, h_t_tilde):\n",
    "        embedding = self.output_lookup(input.view(1, 1))\n",
    "        # context_embedding = torch.cat((embedding, h_t_tilde), dim=-1)\n",
    "\n",
    "        # lstm\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "\n",
    "        y = self.softmax(self.dense_out(output))\n",
    "        return y, hidden, h_t_tilde, \\\n",
    "            (None, None)  # for conformed interfacing w/ Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, name: str, vocab: Vocab, embedding_size: int,\n",
    "                 n_hidden: int, lstm_layers: int, local_window):\n",
    "        \"\"\"\n",
    "        Decoder Module with Attention Mechanism for neural machine translation\n",
    "        :param name: name for object instance\n",
    "        :param vocab: output vocabulary for predictions\n",
    "        :param embedding_size:\n",
    "        :param n_hidden:\n",
    "        :param lstm_layers:\n",
    "        :param local_window:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.__name__ = name\n",
    "\n",
    "        n_pt_weights = n_hidden\n",
    "\n",
    "        # Saving this so that other parts of the class can re-use it\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = lstm_layers\n",
    "        self.out_lut = vocab.tensor2tokens\n",
    "        self.local_window = local_window\n",
    "\n",
    "        # word embeddings:\n",
    "        self.output_lookup = nn.Embedding(num_embeddings=vocab.size,\n",
    "                                          embedding_dim=embedding_size)\n",
    "\n",
    "        # attention module\n",
    "        self.p_t_dense = nn.Linear(self.n_hidden, n_pt_weights, bias=False)\n",
    "        self.p_t_dot = nn.Linear(n_pt_weights, 1, bias=False)\n",
    "        self.score = nn.Bilinear(self.n_hidden, self.n_hidden, 1, bias=False)  # ?\n",
    "\n",
    "        self.combine_attention = nn.Linear(2 * self.n_hidden, self.n_hidden)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size + self.n_hidden,\n",
    "                            hidden_size=self.n_hidden,\n",
    "                            num_layers=self.n_layers,\n",
    "                            bidirectional=False,\n",
    "                            dropout=0.2)\n",
    "\n",
    "        self.dense_out = nn.Linear(self.n_hidden, vocab.size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input, hidden, h_s, h_t_tilde):\n",
    "        embedding = self.output_lookup(input.view(1, 1))\n",
    "\n",
    "        context_embedding = torch.cat((embedding, h_t_tilde), dim=-1)\n",
    "\n",
    "        # lstm\n",
    "        output, hidden = self.lstm(context_embedding, hidden)\n",
    "\n",
    "        # attention\n",
    "        if self.local_window:\n",
    "            # local\n",
    "            h_t = hidden[0][-1]\n",
    "            p_t = h_s.size(0) * torch.sigmoid(self.p_t_dot(torch.tanh(self.p_t_dense(h_t)))).squeeze()  # (9)\n",
    "            s = torch.round(p_t).long().cuda()\n",
    "            D = self.local_window\n",
    "            minimum, maximum = max(s - D, 0), min(s + D, h_s.size(0) - 1)\n",
    "            h_s_local = h_s[minimum:maximum + 1]  # @@@@@ zero pad? @@@@@\n",
    "            h_t_rep = h_t.repeat(h_s_local.size(0), 1, 1)\n",
    "            score = self.score(h_t_rep, h_s_local)  # (8) (general)\n",
    "            gauss_window = torch.exp((torch.arange(minimum, maximum + 1).float() - p_t) ** 2 / (D / 2) ** 2).view(-1, 1, 1).cuda()\n",
    "            a_t = torch.softmax(score, dim=0) * gauss_window  # (7) & (10)\n",
    "            context = torch.mean(a_t * h_s_local, dim=0, keepdim=True)\n",
    "        else:\n",
    "            # global\n",
    "            pass\n",
    "\n",
    "        h_t_tilde = torch.tanh(self.combine_attention(torch.cat((context, h_t.view(1, 1, -1)), dim=-1)))  # (5)\n",
    "        y = self.softmax(self.dense_out(h_t_tilde))  # (6)\n",
    "\n",
    "        return y, hidden, h_t_tilde, (a_t, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMT:\n",
    "\n",
    "    def __init__(self, name, encoder, decoder, loss_fn):\n",
    "        \"\"\"\n",
    "        simple wrapper class for handling encoder-decoder structure.\n",
    "        :param name: Name for NMT instance\n",
    "        :param encoder: Encoder module instance\n",
    "        :param decoder: Decoder module instance (must match hidden state size of encoder)\n",
    "        :param loss_fn: a loss function for comparison with log-softmax layer. Probably NLLLoss(dim=-1)\n",
    "        \"\"\"\n",
    "        self.__name__ = name\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optim.Adam((*encoder.parameters(), *decoder.parameters()), lr=1e-3)\n",
    "        self.epochs = 0\n",
    "\n",
    "    def set_optimizer(self, optimizer, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        explicitly set the optimizer\n",
    "        :param optimizer: new optimizer\n",
    "        :param learning_rate: new learning rate\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer((*self.encoder.parameters(),\n",
    "                                    *self.decoder.parameters()),\n",
    "                                   lr=learning_rate)\n",
    "\n",
    "    def train(self, X, y, epochs=1, batch_size=1, clipping=0.25, print_every=4, examples=[],\n",
    "              examples_epoch_fn=lambda x: True):\n",
    "        \"\"\"\n",
    "        epoch training function\n",
    "        :param X: input training data\n",
    "        :param y: output training data\n",
    "        :param epochs: number of epochs to train\n",
    "        :param batch_size: batch size (WARNING: Empirically doesn't work)\n",
    "        :param clipping: gradient clipping coef\n",
    "        :param print_every: print loss at every N% completion of each epoch\n",
    "        :param examples: optional training examples for displaying progress.\n",
    "        :param examples_epoch_fn: function which takes the epoch number, and returns a boolean value.\n",
    "        Training examples are only printed for epochs where the function returns true.\n",
    "        e.g. lambda x: x**(1/2) == int(x**(1/2))\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        N = len(X)\n",
    "\n",
    "        for e in range(epochs):\n",
    "            print_loss_total = prev_log_iter = prev_log_percent = 0\n",
    "            for n in range(N):\n",
    "                xi = X[n]\n",
    "                yi = y[n]\n",
    "                step = True\n",
    "                print_loss_total += self._train(xi.cuda(), yi.cuda(), clipping, step=step, loss_factor=batch_size) * batch_size\n",
    "                if step and 100 * n // N >= prev_log_percent + print_every:\n",
    "                    print_loss_avg = print_loss_total / (n - prev_log_iter)\n",
    "                    prev_log_iter = n\n",
    "                    prev_log_percent = 100 * n // N\n",
    "                    print('\\repoch %s: %%%s complete     avg loss: %.4f' %\n",
    "                          (self.epochs, prev_log_percent, print_loss_avg))\n",
    "                    print_loss_total = 0\n",
    "            self.epochs += 1\n",
    "            if examples_epoch_fn(e):\n",
    "                for ex in examples:\n",
    "                    output = self.predict(ex)\n",
    "                    print(' '.join(self.decoder.out_lut(output)))\n",
    "\n",
    "    def predict(self, input, cap: int = 20):\n",
    "        \"\"\"\n",
    "        returns prediction vector from input\n",
    "        :param input: input statement\n",
    "        :param cap: max length for prediction.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        hidden = self.encoder.init_hidden()\n",
    "        encoded, hidden = self.encoder(input.view(-1, 1), hidden)\n",
    "        inp = torch.tensor([[1]]).cuda()  # EOS_TOKEN\n",
    "        h_t_tilde = hidden[0][-1].unsqueeze(0) * 0\n",
    "        output = []\n",
    "        for di in range(cap):\n",
    "            logits, hidden, h_t_tilde, decoder_attention = self.decoder(inp, hidden, encoded, h_t_tilde)\n",
    "            topv, topi = logits.topk(1)\n",
    "            inp = topi.squeeze().detach()  # detach from history as input\n",
    "            if inp.item() == 1:  # EOS_TOKEN\n",
    "                break\n",
    "            output.append(inp.squeeze())\n",
    "\n",
    "        return torch.stack(tuple(output), dim=0)\n",
    "\n",
    "    def save(self, path=None):\n",
    "        if not path:\n",
    "            path = os.path.join('.', self.name)\n",
    "        encoder = path + '_encoder'\n",
    "        decoder = path + '_decoder'\n",
    "        torch.save(self.encoder.state_dict(), encoder)\n",
    "        torch.save(self.decoder.state_dict(), decoder)\n",
    "\n",
    "    def load(self, path):\n",
    "        encoder = path + '_encoder'\n",
    "        decoder = path + '_decoder'\n",
    "        enc = torch.load(encoder)\n",
    "        dec = torch.load(decoder)\n",
    "        self.encoder.load_state_dict(enc)\n",
    "        self.decoder.load_state_dict(dec)\n",
    "        \n",
    "    def _train(self, x, y, clipping, step, loss_factor=1):\n",
    "        \"\"\"\n",
    "        single example training\n",
    "        :param x: single input sentence vector\n",
    "        :param y: single output sentence vector\n",
    "        :param clipping: gradient clipping\n",
    "        :param step: execute optimizer step or not\n",
    "        :param loss_factor: basic factor for batch averaging.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        hidden = self.encoder.init_hidden()\n",
    "\n",
    "        target_length = y.size(0)\n",
    "\n",
    "        loss = 0\n",
    "        x = x.view(-1, 1)\n",
    "        h_s, hidden = self.encoder(x, hidden)\n",
    "\n",
    "        bos = torch.tensor(\n",
    "            [[0]]).cuda()  # BOS_TOKEN\n",
    "        inp = bos\n",
    "        h_t_tilde = hidden[0][-1].unsqueeze(0) * 0\n",
    "        for di in range(target_length):\n",
    "            logits, hidden, h_t_tilde, decoder_attention = self.decoder(inp, hidden, h_s, h_t_tilde)\n",
    "            topv, topi = logits.topk(1)\n",
    "            inp = topi.squeeze().detach()  # detach from history as input\n",
    "            loss += self.loss_fn(logits.view(1, -1), y[di].view(1)) / loss_factor\n",
    "            if inp.item() == 1:  # EOS_TOKEN\n",
    "                break\n",
    "            inp = y[di]\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_((*self.encoder.parameters(), *self.decoder.parameters()), clipping)\n",
    "\n",
    "        if step:\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder('encoder', vocab=en_lang, embedding_size=100, n_hidden=70, lstm_layers=2)\n",
    "decoder = Decoder('decoder', vocab=es_lang, embedding_size=100, n_hidden=70, lstm_layers=2, local_window=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt = NMT('local_nmt', encoder, decoder, nn.NLLLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmt.train(X_train, y_train, epochs=100, batch_size=1, examples=X_test[:5])\n",
    "nmt.load('nmt_local_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_test.pkl', 'rb') as fd:\n",
    "    X_test = pickle.load(fd)\n",
    "with open('y_test.pkl', 'rb') as fd:\n",
    "    y_test = pickle.load(fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in:    doctors dispensaries and social support structures are needed\n",
      "trans: que la comisión ha sido el sr prodi y que la comisión ha sido el sr prodi y la comisión\n",
      "true:  se necesitan médicos ambulatorios estructuras sociales complementarias\n",
      "\n",
      "in:    this would show respect for these peripheral areas which are rather concerned about enlargement to the east\n",
      "trans: que la comisión ha sido en el parlamento europeo y la comisión ha sido un informe de la comisión y\n",
      "true:  ello sería dar muestra de respeto hacia esas zonas periféricas algo inquietas con la ampliación hacia el este\n",
      "\n",
      "in:    europe must i feel today establish a genuine codevelopment policy related to migratory flows which should mr president centre on a few simple proposals such as the framing of aid policies for projects by migrants devising instruments for channelling the savings of immigrants the placement of students in training and the placement of young trainees\n",
      "trans: que la comisión ha sido un instrumento que se ha presentado a la comisión de asuntos de la unión europea\n",
      "true:  europa me parece debe establecer hoy una política de codesarrollo ligada a los flujos migratorios que debería articularse señor presidente sobre algunas propuestas simples tales como la elaboración de políticas de ayuda a proyectos de los emigrantes la elaboración de instrumentos que permitan canalizar su ahorro la acogida de estudiantes en formación y la acogida de jóvenes en prácticas\n",
      "\n",
      "in:    the report was full of sound and fury signifying nothing\n",
      "trans: la comisión de la unión europea y de la comisión de la unión europea y de la comisión y la\n",
      "true:  el informe estaba lleno de mucho ruido y pocas nueces\n",
      "\n",
      "in:    far from being satisfactory i see it merely as a first step although it does in some respects go in the right direction\n",
      "trans: que se ha presentado a la comisión que se ha presentado la comisión que se ha presentado a la comisión\n",
      "true:  lejos de ser satisfactoria constituye a mi juicio un primer paso que en ciertos aspectos va no obstante bien orientada\n",
      "\n",
      "in:    in my opinion the message which the european parliament should give president wahid and in this sense i hope i can reassure mrs maijweggen is that we will remain critical of the progress and speed of reforms the status of human rights and the way in which the government protects the citizens on the moluccas\n",
      "trans: que la comisión ha sido en el consejo y la comisión ha sido en el caso de la comisión y\n",
      "true:  el mensaje que el parlamento europeo en lo que a mí respecta debería transmitir al presidente wahid y en este sentido espero tranquilizar a la colega maijweggen es que por una parte debemos continuar siendo críticos respecto del avance y ritmo de las reformas sobre el estado de los derechos humanos y sobre la forma en que el gobierno protege a los ciudadanos de las molucas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (en, es) in enumerate(zip(X_test, y_test)):\n",
    "    en_sent = en_lang.tensor2tokens(en)\n",
    "    es_sent = decoder.out_lut(es)\n",
    "    pred_es = nmt.predict(en)\n",
    "    pred_es_sent = decoder.out_lut(pred_es)\n",
    "    if i > 5:\n",
    "        break\n",
    "    print(\"in:    %s\\ntrans: %s\\ntrue:  %s\\n\" % (' '.join(en_sent), ' '.join(pred_es_sent), ' '.join(es_sent)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/euler/anaconda3/envs/nlp/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/euler/anaconda3/envs/nlp/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/euler/anaconda3/envs/nlp/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i, (en, es) in enumerate(zip(X_test, y_test)):\n",
    "    # en_sent = en_lang.tensor2tokens(en)\n",
    "    es_sent = decoder.out_lut(es)\n",
    "    pred_es = nmt.predict(en)\n",
    "    pred_es_sent = decoder.out_lut(pred_es)\n",
    "    scores.append(bleu_score.sentence_bleu(pred_es_sent, es_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
